{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generate Policies for Natural Disaster Simulation\n",
    "\n",
    "This notebook generates policies for the natural disaster relief experiment from the paper. For each of the 10000 policies, we record the rewards for each of the 12 clusters used in hte experiment. The policies are stored in 'data/natural_disaster/policy_rewards.csv'."
   ],
   "id": "b227afdfb2907837"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T18:30:18.685829Z",
     "start_time": "2025-06-15T18:30:18.302905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.environments.natural_disaster import (\n",
    "    need_based_policy,\n",
    "    per_capita_need_policy,\n",
    "    population_based_policy,\n",
    "    income_based_policy,\n",
    "    proximity_based_policy,\n",
    "    randomized_weighted_hybrid_policy,\n",
    "    mixed_random_policy_k_increments,\n",
    "    generate_action_space,\n",
    "    simulate_policy_dynamic_with_tpm\n",
    ")\n",
    "import csv\n",
    "import os"
   ],
   "id": "bab4239bd8faeea0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T23:21:40.438315Z",
     "start_time": "2025-06-15T23:15:11.002864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Experimental setup for fixed population size and need\n",
    "clusters = [\n",
    "    {\"id\": 1, \"density\": \"High\", \"proximity\": \"Far\", \"income\": \"High-Income\", \"population\": 148, \"initial_need\": 100},\n",
    "    {\"id\": 2, \"density\": \"High\", \"proximity\": \"Far\", \"income\": \"Low-Income\", \"population\": 307, \"initial_need\": 300},\n",
    "    {\"id\": 3, \"density\": \"High\", \"proximity\": \"Far\", \"income\": \"Middle-Income\", \"population\": 616, \"initial_need\": 200},\n",
    "    {\"id\": 4, \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"High-Income\", \"population\": 816, \"initial_need\": 50},\n",
    "    {\"id\": 5, \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"Low-Income\", \"population\": 1405, \"initial_need\": 200},\n",
    "    {\"id\": 6, \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"Middle-Income\", \"population\": 2782,\n",
    "     \"initial_need\": 300},\n",
    "    {\"id\": 7, \"density\": \"Low\", \"proximity\": \"Far\", \"income\": \"High-Income\", \"population\": 74, \"initial_need\": 100},\n",
    "    {\"id\": 8, \"density\": \"Low\", \"proximity\": \"Far\", \"income\": \"Low-Income\", \"population\": 203, \"initial_need\": 500},\n",
    "    {\"id\": 9, \"density\": \"Low\", \"proximity\": \"Far\", \"income\": \"Middle-Income\", \"population\": 396, \"initial_need\": 350},\n",
    "    {\"id\": 10, \"density\": \"Low\", \"proximity\": \"Near\", \"income\": \"High-Income\", \"population\": 36, \"initial_need\": 50},\n",
    "    {\"id\": 11, \"density\": \"Low\", \"proximity\": \"Near\", \"income\": \"Low-Income\", \"population\": 113, \"initial_need\": 50},\n",
    "    {\"id\": 12, \"density\": \"Low\", \"proximity\": \"Near\", \"income\": \"Middle-Income\", \"population\": 230, \"initial_need\": 50}\n",
    "]\n",
    "\n",
    "# Allocation Parameters\n",
    "K = 150  # Total additional units to allocate\n",
    "k = 50  # Allocation increment\n",
    "\n",
    "# MDP Parameters\n",
    "horizon = 3  # Number of time steps\n",
    "initial_state = tuple([cluster['initial_need'] for cluster in clusters])\n",
    "p = 0.7\n",
    "num_clusters = len(clusters)\n",
    "\n",
    "new_clusters = []\n",
    "for adict in clusters:\n",
    "    adict2 = adict.copy()\n",
    "    adict2['initial_need'] += k * horizon\n",
    "    new_clusters.append(adict2)\n",
    "\n",
    "policy_functions = {\n",
    "    \"need_based\": need_based_policy,\n",
    "    \"per_capita\": per_capita_need_policy,\n",
    "    \"population_based\": population_based_policy,\n",
    "    \"income_based\": income_based_policy,\n",
    "    \"proximity_based\": proximity_based_policy,\n",
    "    \"weighted_hybrid\": randomized_weighted_hybrid_policy,  # Add this line\n",
    "    \"mixed_random\": mixed_random_policy_k_increments\n",
    "}\n",
    "\n",
    "policy_functions_list = [i for i in policy_functions.values()]\n",
    "\n",
    "# Step 1: Generate Action Space\n",
    "print('generating actions')\n",
    "action_space = generate_action_space(num_clusters, k, K)\n",
    "\n",
    "# Define parameters\n",
    "epsilon = 0.01  # Include only states with probability > 0.01\n",
    "################################################################\n",
    "# Generate 1000 different policies for the simulation\n",
    "num_simulations = 10000\n",
    "simulation_results = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    rewards, policy = simulate_policy_dynamic_with_tpm(\n",
    "        initial_state=initial_state,\n",
    "        clusters=new_clusters,\n",
    "        k=k,\n",
    "        K=k,\n",
    "        p=p,\n",
    "        horizon=horizon,\n",
    "        action_space=action_space,\n",
    "        policy_functions=policy_functions,\n",
    "        epsilon=0.01\n",
    "    )\n",
    "    simulation_results.append({\"simulation\": i + 1, \"rewards\": rewards, \"policy\": policy})\n",
    "\n",
    "# Print rewards for the first few simulations\n",
    "for result in simulation_results[:10]:\n",
    "    print(f\"Simulation {result['simulation']} -> Rewards: {result['rewards']}\")\n",
    "\n",
    "output_csv = os.path.join('..', '..', 'data', 'natural_disaster', 'policy_rewards.csv')\n",
    "\n",
    "# Open the file for writing\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow([f\"Cluster_{i + 1}_Reward\" for i in range(num_clusters)])\n",
    "\n",
    "    # Write only the reward vectors\n",
    "    for result in simulation_results:\n",
    "        writer.writerow(result[\"rewards\"])"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating actions\n",
      "Simulation 1 -> Rewards: [0.030101652892562048, 0.23323278236914724, 0.021092089728453337, 0.03691115702479354, 0.02177922077922075, 0.06620247933884309, 0.029910743801652956, 0.349394681076499, 0.02116694214876014, 0.0385723140495869, 0.03691115702479354, 0.03691115702479354]\n",
      "Simulation 2 -> Rewards: [0.03050578512396701, 0.01668457300275475, 0.02124439197166467, 0.04479132231404977, 0.02416469893742617, 0.3822782369145995, 0.03731487603305747, 0.24935134562407316, 0.015185950413223171, 0.0452066115702481, 0.03738533057851255, 0.03717768595041338]\n",
      "Simulation 3 -> Rewards: [0.0350239669421484, 0.02082093663911839, 0.021587957497048376, 0.03717768595041338, 0.026340613931522993, 0.3321046831955913, 0.030696694214876103, 0.07930959949141762, 0.22701005509641806, 0.0526126033057853, 0.04479132231404977, 0.04499896694214894]\n",
      "Simulation 4 -> Rewards: [0.03015371900826453, 0.016701101928374585, 0.0212656434474616, 0.045451446280991896, 0.03266056670602123, 0.37627823691459944, 0.03995371900826409, 0.24302489934308127, 0.015043388429752098, 0.04379028925619849, 0.04275206611570264, 0.037630165289256355]\n",
      "Simulation 5 -> Rewards: [0.03000000000000007, 0.02160881542699717, 0.021428571428571398, 0.03874586776859519, 0.13493880362062086, 0.07086914600550964, 0.04228181818181777, 0.4502335240517061, 0.01515743801652896, 0.03812293388429768, 0.03770764462809934, 0.03750000000000017]\n",
      "Simulation 6 -> Rewards: [0.030161157024793456, 0.01702754820936632, 0.021270956316410832, 0.04483780991735556, 0.021442739079102685, 0.05914325068870542, 0.03035206611570255, 0.5632492053401139, 0.01567685950413224, 0.037639462809917514, 0.037431818181818344, 0.037224173553719174]\n",
      "Simulation 7 -> Rewards: [0.04037272727272689, 0.01666666666666659, 0.021428571428571398, 0.04857438016528942, 0.021428571428571398, 0.17225482093664024, 0.0313363636363637, 0.44374592074592095, 0.015000000000000034, 0.044698347107438155, 0.043660123966942306, 0.03791528925619851]\n",
      "Simulation 8 -> Rewards: [0.044337190082644244, 0.016696969696969623, 0.02143211334120422, 0.0501487603305787, 0.13494234553325374, 0.05757713498622601, 0.03071900826446288, 0.44116009747827883, 0.024695867768594837, 0.038243801652892706, 0.03720557851239686, 0.0376208677685952]\n",
      "Simulation 9 -> Rewards: [0.030572727272727345, 0.025100550964187254, 0.021428571428571398, 0.03770764462809934, 0.021943919716646956, 0.07967906336088136, 0.03095454545454553, 0.5383318499682126, 0.01547231404958681, 0.04919731404958693, 0.03750000000000017, 0.03750000000000017]\n",
      "Simulation 10 -> Rewards: [0.02952892561983477, 0.016566115702479272, 0.025844746162927954, 0.03732644628099188, 0.04233589138134592, 0.28797245179063263, 0.1560592286501369, 0.23065490570036037, 0.014764462809917386, 0.03732644628099188, 0.03711880165289271, 0.03691115702479354]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff0860589b207a67"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
