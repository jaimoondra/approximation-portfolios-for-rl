{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Import statements\n",
    "\n",
    "from itertools import product\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Aggregate preferences into single reward\n",
    "def compute_single_reward(state, action, next_state, clusters, cluster_idx, k, K, horizon):\n",
    "    \"\"\"\n",
    "    Computes the reward for a single cluster.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): Current state of unmet needs.\n",
    "        action (tuple): Action taken.\n",
    "        next_state (tuple): Next state of unmet needs.\n",
    "        cluster_idx (int): Index of the cluster.\n",
    "        k (int): Increment size for unmet need increase.\n",
    "\n",
    "    Returns:\n",
    "        float: Reward for the specified cluster.\n",
    "    \"\"\"\n",
    "    initial_need = clusters[cluster_idx]['initial_need']\n",
    "    allocation = action[cluster_idx]\n",
    "    increase = max(0, next_state[cluster_idx] - state[cluster_idx]) if action[cluster_idx] == 0 else 0\n",
    "    immed_rwd = max(k/10, allocation - increase)\n",
    "    frac_alleviated = immed_rwd/initial_need\n",
    "    # return frac_alleviated + allocation/(horizon*K)\n",
    "    return .5*allocation/(horizon*K) + .5*frac_alleviated\n",
    "\n",
    "\n",
    "### Different policy types\n",
    "def need_based_policy(state, action_space, cluster_data, k, K):\n",
    "    max_need_idx = max(range(len(state)), key=lambda i: state[i])\n",
    "    for action in action_space:\n",
    "        if action[max_need_idx] == K:  # Allocate all K units to the cluster\n",
    "            return action\n",
    "    return random.choice(action_space)  # Fallback (shouldn't happen if action space is correct)\n",
    "\n",
    "def per_capita_need_policy(state, action_space, cluster_data, k, K):\n",
    "    per_capita_needs = [\n",
    "        state[i] / cluster_data[i][\"population\"] if cluster_data[i][\"population\"] > 0 else 0\n",
    "        for i in range(len(state))\n",
    "    ]\n",
    "    max_per_capita_idx = per_capita_needs.index(max(per_capita_needs))\n",
    "    for action in action_space:\n",
    "        if action[max_per_capita_idx] == K:  # Allocate all K units to the cluster\n",
    "            return action\n",
    "    return random.choice(action_space)  # Fallback\n",
    "\n",
    "def population_based_policy(state, action_space, cluster_data, k, K):\n",
    "    max_population_idx = max(range(len(state)), key=lambda i: cluster_data[i][\"population\"])\n",
    "    for action in action_space:\n",
    "        if action[max_population_idx] == K:  # Allocate all K units to the cluster\n",
    "            return action\n",
    "    return random.choice(action_space)  # Fallback\n",
    "\n",
    "def income_based_policy(state, action_space, cluster_data, k, K):\n",
    "    income_priority = {\"Low-Income\": 0, \"Middle-Income\": 1, \"High-Income\": 2}\n",
    "    sorted_indices = sorted(\n",
    "        range(len(state)),\n",
    "        key=lambda i: (income_priority[cluster_data[i][\"income\"]], -state[i])\n",
    "    )\n",
    "    for idx in sorted_indices:\n",
    "        for action in action_space:\n",
    "            if action[idx] == K:  # Allocate all K units to the cluster\n",
    "                return action\n",
    "    return random.choice(action_space)  # Fallback\n",
    "\n",
    "def proximity_based_policy(state, action_space, cluster_data, k, K):\n",
    "    proximity_priority = {\"Near\": 0, \"Far\": 1}\n",
    "    sorted_indices = sorted(\n",
    "        range(len(state)),\n",
    "        key=lambda i: (proximity_priority[cluster_data[i][\"proximity\"]], -state[i])\n",
    "    )\n",
    "    for idx in sorted_indices:\n",
    "        for action in action_space:\n",
    "            if action[idx] == K:  # Allocate all K units to the cluster\n",
    "                return action\n",
    "    return random.choice(action_space)  # Fallback\n",
    "\n",
    "def weighted_hybrid_policy(state, action_space, cluster_data, k, K, weights):\n",
    "    \"\"\"\n",
    "    Combines multiple rules using weighted scoring.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): Current unmet needs for each cluster.\n",
    "        action_space (list): List of feasible actions.\n",
    "        cluster_data (list): Cluster characteristics.\n",
    "        k (int): Increment size for allocations.\n",
    "        K (int): Total allocation budget.\n",
    "        weights (dict): Weights for different criteria.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Action vector based on the weighted hybrid policy.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] > 0:  # Only score clusters with unmet need\n",
    "            need_score = state[i] * weights.get(\"need\", 0)\n",
    "            per_capita_score = (state[i] / cluster_data[i][\"population\"]) * weights.get(\"per_capita\", 0)\n",
    "            income_score = weights.get(\"income\", 0) * (1 if cluster_data[i][\"income\"] == \"Low-Income\" else 0)\n",
    "            proximity_score = weights.get(\"proximity\", 0) * (1 if cluster_data[i][\"proximity\"] == \"Near\" else 0)\n",
    "            total_score = need_score + per_capita_score + income_score + proximity_score\n",
    "        else:\n",
    "            total_score = float('-inf')  # Ignore clusters with zero unmet need\n",
    "        scores.append(total_score)\n",
    "\n",
    "    # Allocate to the cluster with the highest score\n",
    "    max_score_idx = scores.index(max(scores))\n",
    "    for action in action_space:\n",
    "        if action[max_score_idx] == K:\n",
    "            return action\n",
    "    return random.choice(action_space)\n",
    "\n",
    "def randomized_weighted_hybrid_policy(state, action_space, cluster_data, k, K):\n",
    "    \"\"\"\n",
    "    Randomized weighted hybrid policy with allocation in increments of k.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): Current unmet needs for each cluster.\n",
    "        action_space (list): List of feasible actions.\n",
    "        cluster_data (list): Cluster characteristics.\n",
    "        k (int): Increment size for allocations.\n",
    "        K (int): Total allocation budget.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Action vector based on the weighted hybrid policy.\n",
    "    \"\"\"\n",
    "    # Randomize weights for each criterion\n",
    "    weights = {\n",
    "        \"need\": random.uniform(0, 1),\n",
    "        \"per_capita\": random.uniform(0, 1),\n",
    "        \"income\": random.uniform(0, 1),\n",
    "        \"proximity\": random.uniform(0, 1),\n",
    "    }\n",
    "    total_weight = sum(weights.values())\n",
    "    weights = {key: value / total_weight for key, value in weights.items()}  # Normalize weights\n",
    "\n",
    "    # Calculate scores for each cluster\n",
    "    scores = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] > 0:  # Only consider clusters with unmet need\n",
    "            # print(state[i])\n",
    "            need_score = state[i] * weights[\"need\"]\n",
    "            per_capita_score = (state[i] / cluster_data[i][\"population\"]) * weights[\"per_capita\"]\n",
    "            income_score = weights[\"income\"] * (1 if cluster_data[i][\"income\"] == \"Low-Income\" else 0)\n",
    "            proximity_score = weights[\"proximity\"] * (1 if cluster_data[i][\"proximity\"] == \"Near\" else 0)\n",
    "            scores.append(need_score + per_capita_score + income_score + proximity_score)\n",
    "        else:\n",
    "            scores.append(float('-inf'))  # Ignore clusters with zero unmet need\n",
    "\n",
    "    # Allocate K in increments of k to top-scoring clusters\n",
    "    action = [0] * len(state)\n",
    "\n",
    "    # Step 1: Find the cluster with the highest score\n",
    "    max_score_idx = scores.index(max(scores))\n",
    "\n",
    "    # Step 2: Allocate K units if possible\n",
    "    # print(state[max_score_idx])\n",
    "    if state[max_score_idx] >= K:\n",
    "        action[max_score_idx] = K\n",
    "        return tuple(action)\n",
    "\n",
    "    # Step 3: Allocate k units to the highest-scoring cluster\n",
    "    action[max_score_idx] = k\n",
    "\n",
    "    # Step 4: Distribute remaining k units randomly\n",
    "    eligible_indices = [i for i in range(len(state)) if state[i] > 0 and i != max_score_idx]\n",
    "    if eligible_indices:\n",
    "        random_idx = random.choice(eligible_indices)\n",
    "        action[random_idx] = k\n",
    "\n",
    "    return tuple(action)\n",
    "\n",
    "def mixed_random_policy_k_increments(state, action_space, cluster_data, k, K, deterministic_share=0.5):\n",
    "    \"\"\"\n",
    "    Mixed random policy that allocates a share deterministically and the rest randomly in increments of k.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): Current unmet needs for each cluster.\n",
    "        action_space (list): List of feasible actions.\n",
    "        cluster_data (list): Cluster characteristics.\n",
    "        k (int): Increment size for allocations.\n",
    "        K (int): Total allocation budget.\n",
    "        deterministic_share (float): Proportion of K to allocate deterministically.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Action vector based on the mixed random policy.\n",
    "    \"\"\"\n",
    "    deterministic_k = round(K * deterministic_share / k) * k\n",
    "    random_k = K - deterministic_k\n",
    "\n",
    "\n",
    "    # Initialize action vector\n",
    "    action = [0] * len(state)\n",
    "\n",
    "    # Deterministic allocation: allocate to the highest need cluster\n",
    "    max_need_idx = max(range(len(state)), key=lambda i: state[i])\n",
    "    if state[max_need_idx] > 0:\n",
    "        max_alloc = min(deterministic_k, (state[max_need_idx] // k) * k)\n",
    "        action[max_need_idx] = max_alloc\n",
    "        deterministic_k -= max_alloc\n",
    "\n",
    "    # Random allocation: distribute remaining units across eligible clusters in increments of k\n",
    "    remaining_k = random_k + deterministic_k  # Include any leftover from deterministic allocation\n",
    "    eligible_indices = [i for i in range(len(state)) if state[i] > 0 and action[i] == 0]\n",
    "\n",
    "    while remaining_k >= k and eligible_indices:\n",
    "        # Randomly select a cluster with unmet need\n",
    "        idx = random.choice(eligible_indices)\n",
    "\n",
    "        # Allocate up to the minimum of k, remaining_k, or the cluster's unmet need in increments of k\n",
    "        max_alloc = min((state[idx] // k) * k, k, remaining_k)\n",
    "        action[idx] += max_alloc\n",
    "        remaining_k -= max_alloc\n",
    "\n",
    "        # Remove cluster from eligible list if fully satisfied\n",
    "        if state[idx] - action[idx] < k:\n",
    "            eligible_indices.remove(idx)\n",
    "\n",
    "    return tuple(action)\n",
    "\n",
    "def apply_policy(policy_name, state, action_space, cluster_data, k, K):\n",
    "    \"\"\"\n",
    "    Applies a specified policy to generate an action.\n",
    "\n",
    "    Args:\n",
    "        policy_name (str): Name of the policy to apply.\n",
    "        state (tuple): Current state of unmet needs.\n",
    "        action_space (list): List of feasible actions.\n",
    "        cluster_data (list): List of cluster characteristics.\n",
    "        k (int): Allocation increment.\n",
    "        K (int): Total allocation budget.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Action vector based on the policy.\n",
    "    \"\"\"\n",
    "    # Map policy names to functions\n",
    "    policy_functions = {\n",
    "    \"need_based\": need_based_policy,\n",
    "    \"per_capita\": per_capita_need_policy,\n",
    "    \"population_based\": population_based_policy,\n",
    "    \"income_based\": income_based_policy,\n",
    "    \"proximity_based\": proximity_based_policy,\n",
    "    \"weighted_hybrid\": randomized_weighted_hybrid_policy,  # Add this line\n",
    "}\n",
    "\n",
    "    policy_func = policy_functions[policy_name]\n",
    "    return policy_func(state, action_space, cluster_data, k, K)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### MDP functions\n",
    "\n",
    "def generate_feasible_next_states(current_state, action, num_clusters, k, p):\n",
    "    \"\"\"\n",
    "    Generates all feasible next states given the current state and action.\n",
    "\n",
    "    Args:\n",
    "        current_state (tuple): Current state (unmet needs of all clusters).\n",
    "        action (tuple): Action (allocation to all clusters).\n",
    "        num_clusters (int): Number of clusters.\n",
    "        k (int): Increment for unmet need increase.\n",
    "        p (float): Probability that unmet need remains unchanged.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (next_state, probability) pairs.\n",
    "    \"\"\"\n",
    "    # Step 1: Apply action to get the interim state\n",
    "    interim_state = tuple(\n",
    "        max(0, current_state[i] - action[i]) for i in range(num_clusters)\n",
    "    )\n",
    "\n",
    "    # Step 2: Identify unmet clusters\n",
    "    unmet_clusters = [i for i in range(num_clusters) if interim_state[i] > 0 and action[i] == 0]\n",
    "\n",
    "    # Step 3: Generate feasible next states\n",
    "    next_states = []\n",
    "    if not unmet_clusters:\n",
    "        # No unmet clusters, state remains unchanged\n",
    "        next_states.append((interim_state, 1.0))\n",
    "    else:\n",
    "        # With probability p, state remains the same\n",
    "        next_states.append((interim_state, p))\n",
    "\n",
    "        # With probability (1-p), one cluster's unmet need increases\n",
    "        for cluster in unmet_clusters:\n",
    "            next_state = list(interim_state)\n",
    "            next_state[cluster] += k\n",
    "            next_states.append((tuple(next_state), (1 - p) / len(unmet_clusters)))\n",
    "\n",
    "    return next_states\n",
    "\n",
    "def generate_action_space(num_clusters, k, K):\n",
    "    \"\"\"\n",
    "    Generates the full feasible action space.\n",
    "\n",
    "    Args:\n",
    "        num_clusters (int): Number of clusters.\n",
    "        k (int): Allocation increment.\n",
    "        K (int): Total allocation budget.\n",
    "\n",
    "    Returns:\n",
    "        list: List of feasible allocation vectors.\n",
    "    \"\"\"\n",
    "    all_possible_actions = product(range(0, K + 1, k), repeat=num_clusters)\n",
    "    feasible_actions = [\n",
    "        action for action in all_possible_actions if sum(action) <= K\n",
    "    ]\n",
    "    return feasible_actions\n",
    "\n",
    "def generate_valid_action(state, action_space):\n",
    "    \"\"\"\n",
    "    Filters the action space to only include actions that allocate to clusters with unmet need > 0.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): Current state of unmet needs.\n",
    "        action_space (list): Precomputed feasible actions.\n",
    "\n",
    "    Returns:\n",
    "        list: Valid actions for the given state.\n",
    "    \"\"\"\n",
    "    valid_actions = []\n",
    "    for action in action_space:\n",
    "        is_valid = True\n",
    "        for i in range(len(state)):\n",
    "            if state[i] == 0 and action[i] > 0:\n",
    "                is_valid = False  # Invalid if allocating to a cluster with no unmet need\n",
    "                break\n",
    "        if is_valid:\n",
    "            valid_actions.append(action)\n",
    "    return valid_actions\n",
    "\n",
    "def generate_state_space(initial_state, horizon, k, num_clusters):\n",
    "    \"\"\"\n",
    "    Generates the feasible state space given the initial state and time horizon.\n",
    "\n",
    "    Args:\n",
    "        initial_state (tuple): Initial unmet needs for all clusters.\n",
    "        horizon (int): Number of time steps.\n",
    "        k (int): Allocation increment.\n",
    "        num_clusters (int): Number of clusters.\n",
    "\n",
    "    Returns:\n",
    "        list: All feasible states over the horizon.\n",
    "    \"\"\"\n",
    "    max_unmet_need = max(initial_state) + (horizon * k)\n",
    "    feasible_states = set([initial_state])\n",
    "\n",
    "    for t in range(horizon):\n",
    "        new_states = set()\n",
    "        for state in feasible_states:\n",
    "            for allocation in product(range(0, k+1, k), repeat=num_clusters):\n",
    "                new_state = tuple(\n",
    "                    max(0, state[i] - allocation[i]) for i in range(num_clusters)\n",
    "                )\n",
    "                new_states.add(new_state)\n",
    "        feasible_states.update(new_states)\n",
    "\n",
    "    return sorted(feasible_states)\n",
    "\n",
    "def generate_sparse_tpm_with_actions(states, action_space, num_clusters, k, p):\n",
    "    \"\"\"\n",
    "    Generates a sparse TPM using precomputed action space.\n",
    "\n",
    "    Args:\n",
    "        states (list): List of all feasible states.\n",
    "        action_space (list): Precomputed feasible action space.\n",
    "        num_clusters (int): Number of clusters.\n",
    "        k (int): Allocation increment.\n",
    "        p (float): Probability that a state remains unchanged if unmet.\n",
    "\n",
    "    Returns:\n",
    "        dict: Sparse TPM, where keys are (current_state, action) pairs\n",
    "              and values are lists of (next_state, probability) tuples.\n",
    "    \"\"\"\n",
    "    state_index = {state: idx for idx, state in enumerate(states)}\n",
    "    tpm = defaultdict(list)\n",
    "\n",
    "    for current_state in states:\n",
    "        for action in action_space:\n",
    "            # Generate interim state\n",
    "            interim_state = tuple(\n",
    "                max(0, current_state[i] - action[i]) for i in range(num_clusters)\n",
    "            )\n",
    "\n",
    "            # Transition probabilities\n",
    "            transitions = defaultdict(float)\n",
    "            unmet_clusters = [i for i in range(num_clusters) if interim_state[i] > 0 and action[i] == 0]\n",
    "\n",
    "            if not unmet_clusters:  # No unmet needs\n",
    "                transitions[interim_state] = 1.0\n",
    "            else:\n",
    "                # With probability p, no unmet need increases\n",
    "                transitions[interim_state] = p\n",
    "                # With probability (1-p), one unmet need increases\n",
    "                for cluster in unmet_clusters:\n",
    "                    next_state = list(interim_state)\n",
    "                    next_state[cluster] += k\n",
    "                    transitions[tuple(next_state)] += (1 - p) / len(unmet_clusters)\n",
    "\n",
    "            # Add transitions to TPM\n",
    "            for next_state, prob in transitions.items():\n",
    "                tpm[(current_state, action)].append((next_state, prob))\n",
    "\n",
    "    return tpm\n",
    "\n",
    "def generate_sparse_tpm_with_scipy(states, action_space, num_clusters, k, p):\n",
    "    \"\"\"\n",
    "    Generates a sparse TPM using scipy's CSR matrix.\n",
    "\n",
    "    Args:\n",
    "        states (list): List of all feasible states.\n",
    "        action_space (list): Precomputed feasible action space.\n",
    "        num_clusters (int): Number of clusters.\n",
    "        k (int): Allocation increment.\n",
    "        p (float): Probability that a state remains unchanged if unmet.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: Sparse TPM matrix of size (num_states, num_states).\n",
    "        dict: Mapping of (state, action) to row indices for the TPM.\n",
    "    \"\"\"\n",
    "    state_index = {state: idx for idx, state in enumerate(states)}\n",
    "    num_states = len(states)\n",
    "\n",
    "    # Lists to store row, column, and value data for the sparse matrix\n",
    "    row_indices = []\n",
    "    col_indices = []\n",
    "    values = []\n",
    "\n",
    "    for current_state in states:\n",
    "        current_idx = state_index[current_state]\n",
    "        for action in action_space:\n",
    "            # Generate interim state\n",
    "            interim_state = tuple(\n",
    "                max(0, current_state[i] - action[i]) for i in range(num_clusters)\n",
    "            )\n",
    "            interim_idx = state_index[interim_state]\n",
    "\n",
    "            # Transition probabilities\n",
    "            transitions = defaultdict(float)\n",
    "            unmet_clusters = [i for i in range(num_clusters) if interim_state[i] > 0 and action[i] == 0]\n",
    "\n",
    "            if not unmet_clusters:  # No unmet needs\n",
    "                transitions[interim_state] = 1.0\n",
    "            else:\n",
    "                # With probability p, no unmet need increases\n",
    "                transitions[interim_state] = p\n",
    "                # With probability (1-p), one unmet need increases\n",
    "                for cluster in unmet_clusters:\n",
    "                    next_state = list(interim_state)\n",
    "                    next_state[cluster] += k\n",
    "                    transitions[tuple(next_state)] += (1 - p) / len(unmet_clusters)\n",
    "\n",
    "            # Add transitions to sparse matrix\n",
    "            for next_state, prob in transitions.items():\n",
    "                next_idx = state_index[next_state]\n",
    "                row_indices.append(current_idx)\n",
    "                col_indices.append(next_idx)\n",
    "                values.append(prob)\n",
    "\n",
    "    # Build the sparse matrix in CSR format\n",
    "    tpm_sparse = csr_matrix((values, (row_indices, col_indices)), shape=(num_states, num_states))\n",
    "\n",
    "    return tpm_sparse, state_index\n",
    "\n",
    "def value_iteration_dynamic(states, actions, num_clusters, k, p, rewards, gamma=0.9, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Value iteration using dynamically generated feasible next states.\n",
    "\n",
    "    Args:\n",
    "        states (list): List of all states.\n",
    "        actions (list): List of all feasible actions.\n",
    "        num_clusters (int): Number of clusters.\n",
    "        k (int): Increment for unmet need increase.\n",
    "        p (float): Probability unmet need remains unchanged.\n",
    "        rewards (dict): Reward for each state.\n",
    "        gamma (float): Discount factor.\n",
    "        epsilon (float): Convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "        dict: Optimal value function for each state.\n",
    "    \"\"\"\n",
    "    V = {state: 0 for state in states}\n",
    "    delta = float('inf')\n",
    "\n",
    "    while delta > epsilon:\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        for state in states:\n",
    "            max_value = float('-inf')\n",
    "            for action in actions:\n",
    "                next_states = generate_feasible_next_states(state, action, num_clusters, k, p)\n",
    "                value = sum(prob * (rewards.get(next_state, 0) + gamma * V[next_state]) for next_state, prob in next_states)\n",
    "                max_value = max(max_value, value)\n",
    "            V_new[state] = max_value\n",
    "            delta = max(delta, abs(V_new[state] - V[state]))\n",
    "        V = V_new\n",
    "\n",
    "    return V\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_random_policy(states, action_space):\n",
    "    \"\"\"\n",
    "    Generates a random policy mapping each state to a random action.\n",
    "\n",
    "    Args:\n",
    "        states (list): List of states.\n",
    "        action_space (list): List of feasible actions.\n",
    "\n",
    "    Returns:\n",
    "        dict: Policy mapping state -> action.\n",
    "    \"\"\"\n",
    "    return {state: random.choice(action_space) for state in states}\n",
    "\n",
    "def compute_expected_values(state, action, next_states, clusters, k, K, horizon, prob):\n",
    "    \"\"\"\n",
    "    Computes the expected value per cluster based on the transition probabilities.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): Current state.\n",
    "        action (tuple): Action taken.\n",
    "        next_states (list): List of (next_state, probability) pairs.\n",
    "        num_clusters (int): Number of clusters.\n",
    "        k (int): Allocation increment.\n",
    "\n",
    "    Returns:\n",
    "        list: Expected value added per cluster.\n",
    "    \"\"\"\n",
    "    num_clusters = len(clusters)\n",
    "    expected_values = [0] * num_clusters\n",
    "\n",
    "    # Normalize rewards based on transition probabilities\n",
    "    for next_state, prob_next in next_states:\n",
    "        for i in range(num_clusters):\n",
    "            expected_values[i] += compute_single_reward(state, action, next_state, clusters, i, k, K, horizon) * prob * prob_next\n",
    "\n",
    "\n",
    "    return expected_values\n",
    "\n",
    "def simulate_policy_dynamic_with_tpm(\n",
    "    initial_state, clusters, k, K, p, horizon, action_space, policy_functions, epsilon=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the total expected reward under dynamically selected structured policies without random sampling.\n",
    "    Includes all next states with probabilities greater than epsilon.\n",
    "\n",
    "    Args:\n",
    "        initial_state (tuple): Initial state of unmet needs.\n",
    "        num_clusters (int): Number of clusters.\n",
    "        k (int): Increment for unmet need increase.\n",
    "        p (float): Probability unmet need remains unchanged.\n",
    "        horizon (int): Number of time steps.\n",
    "        action_space (list): Precomputed feasible actions.\n",
    "        policy_functions_list (list): List of structured policy functions to select from.\n",
    "        epsilon (float): Threshold for including next states based on probability.\n",
    "\n",
    "    Returns:\n",
    "        dict: Total expected rewards for each cluster.\n",
    "        dict: Final expanded policy mapping state -> action.\n",
    "    \"\"\"\n",
    "    num_clusters = len(clusters)\n",
    "    policy = {}  # Initialize an empty policy\n",
    "    rewards = [0] * num_clusters  # Initialize rewards per cluster\n",
    "    visited_states = set()  # Track visited states\n",
    "    active_states = [(initial_state, 1.0)]  # Start with the initial state\n",
    "\n",
    "    for t in range(horizon):\n",
    "        new_active_states = []  # Track new states reachable in this time step\n",
    "\n",
    "        for state, prob in active_states:\n",
    "            # Generate valid actions for the current state\n",
    "            valid_actions = generate_valid_action(state, action_space)\n",
    "\n",
    "            if state not in policy:\n",
    "                # Dynamically select a policy function for this state\n",
    "                selected_policy = random.choice(list(policy_functions.values()))\n",
    "                policy[state] = selected_policy(state, valid_actions, clusters, k, K)\n",
    "\n",
    "            # Get the action from the policy\n",
    "            action = policy[state]\n",
    "\n",
    "            # Compute the feasible next states\n",
    "            next_states = generate_feasible_next_states(state, action, num_clusters, k, p)\n",
    "\n",
    "            # Compute the expected values per cluster based on next states\n",
    "            expected_values_list = compute_expected_values(state, action, next_states, clusters, k, K, horizon, prob)\n",
    "\n",
    "            # Update rewards with proper normalization\n",
    "            for i in range(num_clusters):\n",
    "                rewards[i] += expected_values_list[i]\n",
    "\n",
    "            # Filter next states based on the probability threshold epsilon\n",
    "            significant_next_states = [\n",
    "                (next_state, prob_next) for next_state, prob_next in next_states if prob_next > epsilon\n",
    "            ]\n",
    "\n",
    "            # Add significant next states to the new active states list\n",
    "            for next_state, prob_next in significant_next_states:\n",
    "                if next_state not in policy:\n",
    "                    # Dynamically select a policy for the next state\n",
    "                    selected_policy_name, selected_policy = random.choice(list(policy_functions.items()))\n",
    "                    policy[next_state] = selected_policy(next_state, valid_actions, clusters, k, K)\n",
    "                if next_state not in visited_states:\n",
    "                    new_active_states.append((next_state, prob * prob_next))  # Update probability for next state\n",
    "\n",
    "            # Mark the current state as visited\n",
    "            visited_states.add(state)\n",
    "\n",
    "        # Update active states for the next time step\n",
    "        active_states = new_active_states\n",
    "\n",
    "    return rewards, policy\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Experimental setup for fixed population size and need\n",
    "clusters = [\n",
    "    {\"id\": 1,  \"density\": \"High\", \"proximity\": \"Far\",  \"income\": \"High-Income\",   \"population\": 148, \"initial_need\": 100},\n",
    "    {\"id\": 2,  \"density\": \"High\", \"proximity\": \"Far\",  \"income\": \"Low-Income\",    \"population\": 307, \"initial_need\": 300},\n",
    "    {\"id\": 3,  \"density\": \"High\", \"proximity\": \"Far\",  \"income\": \"Middle-Income\", \"population\": 616, \"initial_need\": 200},\n",
    "    {\"id\": 4,  \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"High-Income\",   \"population\": 816, \"initial_need\": 50},\n",
    "    {\"id\": 5,  \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"Low-Income\",    \"population\": 1405,\"initial_need\": 200},\n",
    "    {\"id\": 6,  \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"Middle-Income\", \"population\": 2782,\"initial_need\": 300},\n",
    "    {\"id\": 7,  \"density\": \"Low\",  \"proximity\": \"Far\",  \"income\": \"High-Income\",   \"population\": 74,  \"initial_need\": 100},\n",
    "    {\"id\": 8,  \"density\": \"Low\",  \"proximity\": \"Far\",  \"income\": \"Low-Income\",    \"population\": 203, \"initial_need\": 500},\n",
    "    {\"id\": 9,  \"density\": \"Low\",  \"proximity\": \"Far\",  \"income\": \"Middle-Income\", \"population\": 396, \"initial_need\": 350},\n",
    "    {\"id\":10, \"density\": \"Low\",  \"proximity\": \"Near\", \"income\": \"High-Income\",   \"population\": 36,  \"initial_need\": 50},\n",
    "    {\"id\":11, \"density\": \"Low\",  \"proximity\": \"Near\", \"income\": \"Low-Income\",    \"population\": 113, \"initial_need\": 50},\n",
    "    {\"id\":12, \"density\": \"Low\",  \"proximity\": \"Near\", \"income\": \"Middle-Income\", \"population\": 230, \"initial_need\": 50}\n",
    "]\n",
    "\n",
    "# Allocation Parameters\n",
    "K = 150  # Total additional units to allocate\n",
    "k = 50   # Allocation increment\n",
    "\n",
    "# MDP Parameters\n",
    "horizon = 3  # Number of time steps\n",
    "initial_state = tuple([cluster['initial_need'] for cluster in clusters])\n",
    "p= 0.7\n",
    "num_clusters = len(clusters)\n",
    "\n",
    "new_clusters = []\n",
    "for adict in clusters:\n",
    "    adict2 = adict.copy()\n",
    "    adict2['initial_need'] += k*horizon\n",
    "    new_clusters.append(adict2)\n",
    "\n",
    "\n",
    "policy_functions = {\n",
    "    \"need_based\": need_based_policy,\n",
    "    \"per_capita\": per_capita_need_policy,\n",
    "    \"population_based\": population_based_policy,\n",
    "    \"income_based\": income_based_policy,\n",
    "    \"proximity_based\": proximity_based_policy,\n",
    "    \"weighted_hybrid\": randomized_weighted_hybrid_policy,  # Add this line\n",
    "    \"mixed_random\": mixed_random_policy_k_increments\n",
    "}\n",
    "\n",
    "policy_functions_list = [i for i in policy_functions.values()]\n",
    "\n",
    "# Step 1: Generate Action Space\n",
    "print('generating actions')\n",
    "action_space = generate_action_space(num_clusters, k, K)\n",
    "\n",
    "# Define parameters\n",
    "epsilon = 0.01  # Include only states with probability > 0.01\n",
    "################################################################\n",
    "# Generate 1000 different policies for the simulation\n",
    "num_simulations = 10000\n",
    "simulation_results = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    rewards, policy = simulate_policy_dynamic_with_tpm(\n",
    "        initial_state=initial_state,\n",
    "        clusters=new_clusters,\n",
    "        k=k,\n",
    "        K=k,\n",
    "        p=p,\n",
    "        horizon=horizon,\n",
    "        action_space=action_space,\n",
    "        policy_functions=policy_functions,\n",
    "        epsilon=0.01\n",
    "    )\n",
    "    simulation_results.append({\"simulation\": i + 1, \"rewards\": rewards, \"policy\": policy})\n",
    "\n",
    "# Print rewards for the first few simulations\n",
    "for result in simulation_results[:10]:\n",
    "    print(f\"Simulation {result['simulation']} -> Rewards: {result['rewards']}\")\n",
    "\n",
    "output_csv = f\"simulation_rewards_{num_simulations}.csv\"\n",
    "\n",
    "# Open the file for writing\n",
    "with open(output_csv, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow([f\"Cluster_{i+1}_Reward\" for i in range(num_clusters)])\n",
    "\n",
    "    # Write only the reward vectors\n",
    "    for result in simulation_results:\n",
    "        writer.writerow(result[\"rewards\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Experimental setup for randomized and large number of policies given randomized need and population size per cluster\n",
    "\n",
    "# Define possible attributes\n",
    "clusters_base = [\n",
    "    {\"id\": 1,  \"density\": \"High\", \"proximity\": \"Far\",  \"income\": \"High-Income\"},\n",
    "    {\"id\": 2,  \"density\": \"High\", \"proximity\": \"Far\",  \"income\": \"Low-Income\"},\n",
    "    {\"id\": 3,  \"density\": \"High\", \"proximity\": \"Far\",  \"income\": \"Middle-Income\"},\n",
    "    {\"id\": 4,  \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"High-Income\"},\n",
    "    {\"id\": 5,  \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"Low-Income\"},\n",
    "    {\"id\": 6,  \"density\": \"High\", \"proximity\": \"Near\", \"income\": \"Middle-Income\"},\n",
    "    {\"id\": 7,  \"density\": \"Low\",  \"proximity\": \"Far\",  \"income\": \"High-Income\"},\n",
    "    {\"id\": 8,  \"density\": \"Low\",  \"proximity\": \"Far\",  \"income\": \"Low-Income\"},\n",
    "    {\"id\": 9,  \"density\": \"Low\",  \"proximity\": \"Far\",  \"income\": \"Middle-Income\"},\n",
    "    {\"id\":10,  \"density\": \"Low\",  \"proximity\": \"Near\", \"income\": \"High-Income\"},\n",
    "    {\"id\":11,  \"density\": \"Low\",  \"proximity\": \"Near\", \"income\": \"Low-Income\"},\n",
    "    {\"id\":12,  \"density\": \"Low\",  \"proximity\": \"Near\", \"income\": \"Middle-Income\"}\n",
    "]\n",
    "\n",
    "# Define reasonable population ranges\n",
    "population_ranges = {\n",
    "    (\"High\", \"Far\"): (100, 1000),\n",
    "    (\"High\", \"Near\"): (500, 3000),\n",
    "    (\"Low\", \"Far\"): (50, 500),\n",
    "    (\"Low\", \"Near\"): (20, 300),\n",
    "}\n",
    "\n",
    "# Need scaling factors based on income levels\n",
    "need_factors = {\n",
    "    \"High-Income\": 0.1,\n",
    "    \"Middle-Income\": 0.3,\n",
    "    \"Low-Income\": 0.5\n",
    "}\n",
    "\n",
    "# Generate 1000 instances, each with 12 clusters\n",
    "all_instances = []\n",
    "\n",
    "for _ in range(1000):  # Generate 1000 dictionaries\n",
    "    instance_clusters = []\n",
    "    for base_cluster in clusters_base:\n",
    "        density = base_cluster[\"density\"]\n",
    "        proximity = base_cluster[\"proximity\"]\n",
    "        income = base_cluster[\"income\"]\n",
    "        \n",
    "        # Get appropriate population range\n",
    "        pop_min, pop_max = population_ranges[(density, proximity)]\n",
    "        population = random.randint(pop_min, pop_max)\n",
    "\n",
    "        # Initial need based on income category and some noise\n",
    "        initial_need = int(population * need_factors[income] + random.gauss(0, 50))\n",
    "        initial_need = max(50, initial_need)  # Ensure non-zero positive need\n",
    "\n",
    "        instance_clusters.append({\n",
    "            \"id\": base_cluster[\"id\"],\n",
    "            \"density\": density,\n",
    "            \"proximity\": proximity,\n",
    "            \"income\": income,\n",
    "            \"population\": population,\n",
    "            \"initial_need\": initial_need\n",
    "        })\n",
    "    \n",
    "    all_instances.append(instance_clusters)\n",
    "# Allocation Parameters\n",
    "K = 150  # Total additional units to allocate\n",
    "k = 50   # Allocation increment\n",
    "\n",
    "policy_functions = {\n",
    "    \"need_based\": need_based_policy,\n",
    "    \"per_capita\": per_capita_need_policy,\n",
    "    \"population_based\": population_based_policy,\n",
    "    \"income_based\": income_based_policy,\n",
    "    \"proximity_based\": proximity_based_policy,\n",
    "    \"weighted_hybrid\": randomized_weighted_hybrid_policy,  # Add this line\n",
    "    \"mixed_random\": mixed_random_policy_k_increments\n",
    "}\n",
    "\n",
    "policy_functions_list = [i for i in policy_functions.values()]\n",
    "\n",
    "for clusters_num, clusters in enumerate(all_instances):\n",
    "\n",
    "    # MDP Parameters\n",
    "    horizon = 3  # Number of time steps\n",
    "    initial_state = tuple([cluster['initial_need'] for cluster in clusters])\n",
    "    p= 0.7\n",
    "    num_clusters = len(clusters)\n",
    "\n",
    "    new_clusters = []\n",
    "    for adict in clusters:\n",
    "        adict2 = adict.copy()\n",
    "        adict2['initial_need'] += k*horizon\n",
    "        new_clusters.append(adict2)\n",
    "\n",
    "\n",
    "    # Step 1: Generate Action Space\n",
    "    print('generating actions')\n",
    "    action_space = generate_action_space(num_clusters, k, K)\n",
    "\n",
    "    # Define parameters\n",
    "    epsilon = 0.01  # Include only states with probability > 0.01\n",
    "    ################################################################\n",
    "    # Generate 1000 different policies for the simulation\n",
    "    num_simulations = 1000\n",
    "    simulation_results = []\n",
    "\n",
    "    for i in range(num_simulations):\n",
    "        rewards, policy = simulate_policy_dynamic_with_tpm(\n",
    "            initial_state=initial_state,\n",
    "            clusters=new_clusters,\n",
    "            k=k,\n",
    "            K=k,\n",
    "            p=p,\n",
    "            horizon=horizon,\n",
    "            action_space=action_space,\n",
    "            policy_functions=policy_functions,\n",
    "            epsilon=0.01\n",
    "        )\n",
    "        simulation_results.append({\"simulation\": i + 1, \"rewards\": rewards, \"policy\": policy})\n",
    "\n",
    "    # Print rewards for the first few simulations\n",
    "    # for result in simulation_results[:10]:\n",
    "    #     print(f\"Simulation {result['simulation']} -> Rewards: {result['rewards']}\")\n",
    "\n",
    "    output_csv = f\"simulation_rewards/simulation_rewards_iter_{clusters_num}_num-simulations_{num_simulations}.csv\"\n",
    "\n",
    "    # Open the file for writing\n",
    "    with open(output_csv, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writerow([f\"Cluster_{i+1}_Reward\" for i in range(num_clusters)])\n",
    "\n",
    "        # Write only the reward vectors\n",
    "        for result in simulation_results:\n",
    "            writer.writerow(result[\"rewards\"])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
